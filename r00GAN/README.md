# Generative Adversarial Networks (GANs)
 
 ## Introduction: 
 Generative Adversarial Networks (GANs) are a class of deep learning algorithms introduced by [Ian Goodfellow et al. 2014](https://arxiv.org/pdf/1406.2661.pdf). They are composed of two deep neural networks, the **generator** $(G)$ and the **discriminator** $(D)$, which are trained simultaneously through a competitive process. The generator aims to produce data that is indistinguishable from real data, while the discriminator aims to distinguish between real data and the data produced by the generator.

 ![](https://www.simplilearn.com/ice9/free_resources_article_thumb/GAN.PNG)

## Mathematical Framework
The GAN framework can be described by a **minimax game** with the following value function 
$$
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$$

Here, $x$ is a sample from the real data distribution $p_{data}(x)$ and $z$ is a sample from a noise distribution $p_z(z)$, typically a Gaussian distribution. $G(z)$ is the generated data and $D(x)$ is the probablity of x to be real data. 

The generator $G$ tries to map $z$ to the data space as $G(z)$. The discriminator $D(x)$ outputs the probability that $x$ came from the real data rather than $G(z)$.

* Maximize the first term: $\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)]$, represents the expectation of the discriminator's predictions on real data. The discriminator tries to maximize this value, meaning it wants to assign high probabilities to real data.
* Minimize the second term, $\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$, represents the expectation of the discriminator's predictions on fake data generated by the generator. The generator tries to minimize this term by fooling the discriminator into thinking its generated data is real, pushing $D(G(z))$ towards 1.
**Note:** we may get unsaturating gradients or undefined value from the second term if the generated data is perfect i.e. $D(G(z))=1$ and $log(1-D(G(z)))=log(0)$. To handle this, we maximize $\mathbb{E}_{z \sim p_z(z)}[\log(D(G(z)))]$. 
## Applications of GANs

* **Synthetic Data Generation:** GANs can generate highly realistic images that are often indistinguishable from real images. This has applications in privacy, art, entertainment, virtual reality, and data augmentation.
* **Image-to-Image Translation:** GANs can translate images from one domain to another, such as turning sketches into colored images, day scenes to night scenes, photos into paintings, MRI to CT or image to segmentation.
* **Super-Resolution:** GANs can increase the resolution of images, enhancing the quality of low-resolution images for better viewing or analysis.
* **Style Transfer:** GANs can apply the style of one image to another, enabling creative modifications and artistic endeavors.
* **Drug Discovery:** In the pharmaceutical industry, GANs can generate molecular structures for new drugs, speeding up the discovery process.

GANs represent a powerful tool in the AI field, capable of generating realistic, high-quality data across various domains. Their development continues to push the boundaries of what's possible in generative models, leading to new applications and improvements in machine learning.

## Training a GAN:

Because GANs are notoriously hard to train and do not have a straightforward convergence criterion like other deep learning models, determining the exact point to stop training involves a mix of a number of factors, depending on the specific goals and constraints of the project.

Training should ideally stop when it reaches a point where the generator produces high-quality data that is indistinguishable from real data, and the discriminator is unable to reliably tell the difference between real and generated data. However, achieving this equilibrium is challenging due to the dynamic nature of GAN training. Here are several practical criteria and considerations for determining when to stop training a GAN:

**Convergence of Loss:** Unlike traditional models, where a decreasing loss indicates improvement, GAN training involves two loss functions that are in opposition. The generator's loss decreases as it gets better at fooling the discriminator, while the discriminator's loss increases when it gets tricked by the generator. However, these losses can oscillate or fail to converge. Training might be stopped when the loss values stabilize, but this stability can be difficult to achieve or interpret.

**Image Quality:** Visual inspection of the generated images is a common approach. Training can be stopped when the images generated by the GAN meet the desired quality and realism. This method, while subjective, is straightforward and aligns directly with many GAN applications.

**Performance on Validation Set:** If you have a labeled validation set, you can measure the discriminator's performance on this set. However, because the generator's objective is to fool the discriminator, high accuracy on the validation set might simply indicate that the discriminator is not being effectively challenged by the generator.

**FID Score:** The Fréchet Inception Distance (FID) score (also known as **Wasserstein-2 distance**) is a more objective and popular metric for evaluating the quality of images generated by GANs. It measures the distance between the feature vectors of real and generated images. Training can be considered complete when the FID score stops improving or reaches a satisfactory value.

A lower FID score indicates that the generated images are more similar to the real images, implying better quality of the generated images. 


$$
FID = ||\mu_r - \mu_g||^2_2 + \mathrm{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r\Sigma_g)^{\frac{1}{2}})
$$

Here:

  - The term $(||\mu_r - \mu_g||^2_2)$ is the squared Euclidean distance between the mean vectors of the real and generated image features. This term quantifies the difference in the central tendency of the two distributions.
  - $Tr(⋅)$ denotes the trace of a matrix (the sum of all diagonal elements). The term inside the trace calculates the difference between the covariances of the real and generated distributions, adjusted by the geometric mean of the two covariances. This term accounts for the spread and correlation of the features in the distributions.

**Mode Collapse:** Paying attention to the diversity of the generated samples is crucial. If the generator starts producing very similar outputs (a phenomenon known as mode collapse), it may indicate that the GAN is overfitting or that the generator is exploiting weaknesses in the discriminator. Training should be reconsidered or stopped to adjust strategies if this occurs. 

**Overfitting:** Monitoring for overfitting is essential. If the generated images begin to too closely resemble the training data, or if the discriminator's performance on a separate validation set worsens over time, it might be time to stop training.

**Resource and Time Constraints:** Practical considerations such as computational resources, time limitations, and the diminishing returns on further training also play a role in deciding when to stop training.

## Challenges:

GANs are a groundbreaking and powerful class of neural networks, but they come with several challenges and limitations that researchers and practitioners face:

* **Mode Collapse:** This occurs when the generator starts producing a limited variety of outputs, even if those outputs are high quality. In extreme cases, the generator might produce the same output over and over again. This problem arises because the generator finds and exploits weaknesses in the discriminator's ability to differentiate between real and generated data, leading to a lack of diversity in the generated samples.

* **Training Instability:** GANs are known for their training difficulties. The simultaneous training of the generator and discriminator can lead to instability, where the performance of the model oscillates or deteriorates over time instead of steadily improving. This instability is often due to the zero-sum game between the generator and discriminator, where improvements in one part can lead to degradation in the other.

**Convergence Problems:** Determining whether a GAN has converged is challenging, as traditional metrics used in other forms of deep learning (such as loss curves) are not always indicative of performance or progress. GANs can appear to be converging based on these metrics while still producing poor-quality outputs.

**Evaluation Difficulties:** Unlike supervised learning models, evaluating the performance of GANs is not straightforward because there is no clear, universally accepted metric. Metrics like Inception Score (IS) and Fréchet Inception Distance (FID) are commonly used but have their limitations and might not capture all aspects of image quality, such as diversity and realism.

**Hyperparameter Sensitivity:** GANs are sensitive to the choice of hyperparameters, including the architecture of the neural networks, learning rates, and the type of optimization algorithm. Finding the right set of hyperparameters can require extensive experimentation and domain knowledge.

**Limited Understanding:** Despite their success, the theoretical understanding of GANs is still limited. This lack of understanding can make it challenging to diagnose problems or to innovate in ways that improve stability and performance.

**Bias and Ethical Concerns:** GANs can inadvertently learn and amplify biases present in their training data. This can lead to ethical concerns, especially when used in applications like facial recognition, content generation, and more, where biased outputs can have real-world consequences.

**High Computational Cost:** Training GANs is computationally intensive and can require significant GPU resources, especially for models that generate high-resolution images. This can limit the accessibility of GAN technology for some researchers and developers.

**Disentanglement:** Achieving disentanglement, or the ability to control specific features of the generated outputs (such as the angle of rotation of an object), is challenging but desirable for many applications. Some progress has been made in this area, but controlling the generation process remains a complex issue.

Despite these challenges, GANs continue to be a rapidly evolving field with significant research dedicated to overcoming these limitations. Advances in architectures, training methods, and theoretical understanding are helping to make GANs more stable, efficient, and accessible.

## Resources: 

* [An Introduction to Generative Adversarial Networks (GANs).](https://youtu.be/OXWvrRLzEaU?si=4GLpv4kdsAVnuKot)
* Building our first simple GAN. [Video](https://youtu.be/OljTVUVzPpM?si=TJjVbVzwHPfQCF81), [Code.](https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/GANs/1.%20SimpleGAN)
